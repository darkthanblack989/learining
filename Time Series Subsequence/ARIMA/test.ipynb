{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b2e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ARIMA in module statsmodels.tsa.arima.model:\n",
      "\n",
      "class ARIMA(statsmodels.tsa.statespace.sarimax.SARIMAX)\n",
      " |  ARIMA(endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, 0), trend=None, enforce_stationarity=True, enforce_invertibility=True, concentrate_scale=False, trend_offset=1, dates=None, freq=None, missing='none', validate_specification=True)\n",
      " |  \n",
      " |  Autoregressive Integrated Moving Average (ARIMA) model, and extensions\n",
      " |  \n",
      " |  This model is the basic interface for ARIMA-type models, including those\n",
      " |  with exogenous regressors and those with seasonal components. The most\n",
      " |  general form of the model is SARIMAX(p, d, q)x(P, D, Q, s). It also allows\n",
      " |  all specialized cases, including\n",
      " |  \n",
      " |  - autoregressive models: AR(p)\n",
      " |  - moving average models: MA(q)\n",
      " |  - mixed autoregressive moving average models: ARMA(p, q)\n",
      " |  - integration models: ARIMA(p, d, q)\n",
      " |  - seasonal models: SARIMA(P, D, Q, s)\n",
      " |  - regression with errors that follow one of the above ARIMA-type models\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like, optional\n",
      " |      The observed time-series process :math:`y`.\n",
      " |  exog : array_like, optional\n",
      " |      Array of exogenous regressors.\n",
      " |  order : tuple, optional\n",
      " |      The (p,d,q) order of the model for the autoregressive, differences, and\n",
      " |      moving average components. d is always an integer, while p and q may\n",
      " |      either be integers or lists of integers.\n",
      " |  seasonal_order : tuple, optional\n",
      " |      The (P,D,Q,s) order of the seasonal component of the model for the\n",
      " |      AR parameters, differences, MA parameters, and periodicity. Default\n",
      " |      is (0, 0, 0, 0). D and s are always integers, while P and Q\n",
      " |      may either be integers or lists of positive integers.\n",
      " |  trend : str{'n','c','t','ct'} or iterable, optional\n",
      " |      Parameter controlling the deterministic trend. Can be specified as a\n",
      " |      string where 'c' indicates a constant term, 't' indicates a\n",
      " |      linear trend in time, and 'ct' includes both. Can also be specified as\n",
      " |      an iterable defining a polynomial, as in `numpy.poly1d`, where\n",
      " |      `[1,1,0,1]` would denote :math:`a + bt + ct^3`. Default is 'c' for\n",
      " |      models without integration, and no trend for models with integration.\n",
      " |      Note that all trend terms are included in the model as exogenous\n",
      " |      regressors, which differs from how trends are included in ``SARIMAX``\n",
      " |      models.  See the Notes section for a precise definition of the\n",
      " |      treatment of trend terms.\n",
      " |  enforce_stationarity : bool, optional\n",
      " |      Whether or not to require the autoregressive parameters to correspond\n",
      " |      to a stationarity process.\n",
      " |  enforce_invertibility : bool, optional\n",
      " |      Whether or not to require the moving average parameters to correspond\n",
      " |      to an invertible process.\n",
      " |  concentrate_scale : bool, optional\n",
      " |      Whether or not to concentrate the scale (variance of the error term)\n",
      " |      out of the likelihood. This reduces the number of parameters by one.\n",
      " |      This is only applicable when considering estimation by numerical\n",
      " |      maximum likelihood.\n",
      " |  trend_offset : int, optional\n",
      " |      The offset at which to start time trend values. Default is 1, so that\n",
      " |      if `trend='t'` the trend is equal to 1, 2, ..., nobs. Typically is only\n",
      " |      set when the model created by extending a previous dataset.\n",
      " |  dates : array_like of datetime, optional\n",
      " |      If no index is given by `endog` or `exog`, an array-like object of\n",
      " |      datetime objects can be provided.\n",
      " |  freq : str, optional\n",
      " |      If no index is given by `endog` or `exog`, the frequency of the\n",
      " |      time-series may be specified here as a Pandas offset or offset string.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  This model incorporates both exogenous regressors and trend components\n",
      " |  through \"regression with ARIMA errors\". This differs from the\n",
      " |  specification estimated using ``SARIMAX`` which treats the trend\n",
      " |  components separately from any included exogenous regressors. The full\n",
      " |  specification of the model estimated here is:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      Y_{t}-\\delta_{0}-\\delta_{1}t-\\ldots-\\delta_{k}t^{k}-X_{t}\\beta\n",
      " |          & =\\epsilon_{t} \\\\\n",
      " |      \\left(1-L\\right)^{d}\\left(1-L^{s}\\right)^{D}\\Phi\\left(L\\right)\n",
      " |      \\Phi_{s}\\left(L\\right)\\epsilon_{t}\n",
      " |          & =\\Theta\\left(L\\right)\\Theta_{s}\\left(L\\right)\\eta_{t}\n",
      " |  \n",
      " |  where :math:`\\eta_t \\sim WN(0,\\sigma^2)` is a white noise process, L\n",
      " |  is the lag operator, and :math:`G(L)` are lag polynomials corresponding\n",
      " |  to the autoregressive (:math:`\\Phi`), seasonal autoregressive\n",
      " |  (:math:`\\Phi_s`), moving average (:math:`\\Theta`), and seasonal moving\n",
      " |  average components (:math:`\\Theta_s`).\n",
      " |  \n",
      " |  `enforce_stationarity` and `enforce_invertibility` are specified in the\n",
      " |  constructor because they affect loglikelihood computations, and so should\n",
      " |  not be changed on the fly. This is why they are not instead included as\n",
      " |  arguments to the `fit` method.\n",
      " |  \n",
      " |  See the notebook `ARMA: Sunspots Data\n",
      " |  <../examples/notebooks/generated/tsa_arma_0.html>`__ and\n",
      " |  `ARMA: Artificial Data <../examples/notebooks/generated/tsa_arma_1.html>`__\n",
      " |  for an overview.\n",
      " |  \n",
      " |  .. todo:: should concentrate_scale=True by default\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> mod = sm.tsa.arima.ARIMA(endog, order=(1, 0, 0))\n",
      " |  >>> res = mod.fit()\n",
      " |  >>> print(res.summary())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ARIMA\n",
      " |      statsmodels.tsa.statespace.sarimax.SARIMAX\n",
      " |      statsmodels.tsa.statespace.mlemodel.MLEModel\n",
      " |      statsmodels.tsa.base.tsa_model.TimeSeriesModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, 0), trend=None, enforce_stationarity=True, enforce_invertibility=True, concentrate_scale=False, trend_offset=1, dates=None, freq=None, missing='none', validate_specification=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, start_params=None, transformed=True, includes_fixed=False, method=None, method_kwargs=None, gls=None, gls_kwargs=None, cov_type=None, cov_kwds=None, return_params=False, low_memory=False)\n",
      " |      Fit (estimate) the parameters of the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          If None, the default is given by Model.start_params.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `start_params` is already transformed. Default is\n",
      " |          True.\n",
      " |      includes_fixed : bool, optional\n",
      " |          If parameters were previously fixed with the `fix_params` method,\n",
      " |          this argument describes whether or not `start_params` also includes\n",
      " |          the fixed parameters, in addition to the free parameters. Default\n",
      " |          is False.\n",
      " |      method : str, optional\n",
      " |          The method used for estimating the parameters of the model. Valid\n",
      " |          options include 'statespace', 'innovations_mle', 'hannan_rissanen',\n",
      " |          'burg', 'innovations', and 'yule_walker'. Not all options are\n",
      " |          available for every specification (for example 'yule_walker' can\n",
      " |          only be used with AR(p) models).\n",
      " |      method_kwargs : dict, optional\n",
      " |          Arguments to pass to the fit function for the parameter estimator\n",
      " |          described by the `method` argument.\n",
      " |      gls : bool, optional\n",
      " |          Whether or not to use generalized least squares (GLS) to estimate\n",
      " |          regression effects. The default is False if `method='statespace'`\n",
      " |          and is True otherwise.\n",
      " |      gls_kwargs : dict, optional\n",
      " |          Arguments to pass to the GLS estimation fit method. Only applicable\n",
      " |          if GLS estimation is used (see `gls` argument for details).\n",
      " |      cov_type : str, optional\n",
      " |          The `cov_type` keyword governs the method for calculating the\n",
      " |          covariance matrix of parameter estimates. Can be one of:\n",
      " |      \n",
      " |          - 'opg' for the outer product of gradient estimator\n",
      " |          - 'oim' for the observed information matrix estimator, calculated\n",
      " |            using the method of Harvey (1989)\n",
      " |          - 'approx' for the observed information matrix estimator,\n",
      " |            calculated using a numerical approximation of the Hessian matrix.\n",
      " |          - 'robust' for an approximate (quasi-maximum likelihood) covariance\n",
      " |            matrix that may be valid even in the presence of some\n",
      " |            misspecifications. Intermediate calculations use the 'oim'\n",
      " |            method.\n",
      " |          - 'robust_approx' is the same as 'robust' except that the\n",
      " |            intermediate calculations use the 'approx' method.\n",
      " |          - 'none' for no covariance matrix calculation.\n",
      " |      \n",
      " |          Default is 'opg' unless memory conservation is used to avoid\n",
      " |          computing the loglikelihood values for each observation, in which\n",
      " |          case the default is 'oim'.\n",
      " |      cov_kwds : dict or None, optional\n",
      " |          A dictionary of arguments affecting covariance matrix computation.\n",
      " |      \n",
      " |          **opg, oim, approx, robust, robust_approx**\n",
      " |      \n",
      " |          - 'approx_complex_step' : bool, optional - If True, numerical\n",
      " |            approximations are computed using complex-step methods. If False,\n",
      " |            numerical approximations are computed using finite difference\n",
      " |            methods. Default is True.\n",
      " |          - 'approx_centered' : bool, optional - If True, numerical\n",
      " |            approximations computed using finite difference methods use a\n",
      " |            centered approximation. Default is False.\n",
      " |      return_params : bool, optional\n",
      " |          Whether or not to return only the array of maximizing parameters.\n",
      " |          Default is False.\n",
      " |      low_memory : bool, optional\n",
      " |          If set to True, techniques are applied to substantially reduce\n",
      " |          memory usage. If used, some features of the results object will\n",
      " |          not be available (including smoothed results and in-sample\n",
      " |          prediction), although out-of-sample forecasting is possible.\n",
      " |          Default is False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ARIMAResults\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mod = sm.tsa.arima.ARIMA(endog, order=(1, 0, 0))\n",
      " |      >>> res = mod.fit()\n",
      " |      >>> print(res.summary())\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.tsa.statespace.sarimax.SARIMAX:\n",
      " |  \n",
      " |  clone(self, endog, exog=None, **kwargs)\n",
      " |      Clone state space model with new data and optionally new specification\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      endog : array_like\n",
      " |          The observed time-series process :math:`y`\n",
      " |      k_states : int\n",
      " |          The dimension of the unobserved state process.\n",
      " |      exog : array_like, optional\n",
      " |          Array of exogenous regressors, shaped nobs x k. Default is no\n",
      " |          exogenous regressors.\n",
      " |      kwargs\n",
      " |          Keyword arguments to pass to the new model class to change the\n",
      " |          model specification.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : MLEModel subclass\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method must be implemented\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize the SARIMAX model.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      These initialization steps must occur following the parent class\n",
      " |      __init__ function calls.\n",
      " |  \n",
      " |  initialize_default(self, approximate_diffuse_variance=None)\n",
      " |      Initialize default\n",
      " |  \n",
      " |  prepare_data(self)\n",
      " |      Prepare data for use in the state space representation\n",
      " |  \n",
      " |  transform_params(self, unconstrained)\n",
      " |      Transform unconstrained parameters used by the optimizer to constrained\n",
      " |      parameters used in likelihood evaluation.\n",
      " |      \n",
      " |      Used primarily to enforce stationarity of the autoregressive lag\n",
      " |      polynomial, invertibility of the moving average lag polynomial, and\n",
      " |      positive variance parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      unconstrained : array_like\n",
      " |          Unconstrained parameters used by the optimizer.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      constrained : array_like\n",
      " |          Constrained parameters used in likelihood evaluation.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the lag polynomial has non-consecutive powers (so that the\n",
      " |      coefficient is zero on some element of the polynomial), then the\n",
      " |      constraint function is not onto the entire space of invertible\n",
      " |      polynomials, although it only excludes a very small portion very close\n",
      " |      to the invertibility boundary.\n",
      " |  \n",
      " |  untransform_params(self, constrained)\n",
      " |      Transform constrained parameters used in likelihood evaluation\n",
      " |      to unconstrained parameters used by the optimizer\n",
      " |      \n",
      " |      Used primarily to reverse enforcement of stationarity of the\n",
      " |      autoregressive lag polynomial and invertibility of the moving average\n",
      " |      lag polynomial.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      constrained : array_like\n",
      " |          Constrained parameters used in likelihood evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      constrained : array_like\n",
      " |          Unconstrained parameters used by the optimizer.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the lag polynomial has non-consecutive powers (so that the\n",
      " |      coefficient is zero on some element of the polynomial), then the\n",
      " |      constraint function is not onto the entire space of invertible\n",
      " |      polynomials, although it only excludes a very small portion very close\n",
      " |      to the invertibility boundary.\n",
      " |  \n",
      " |  update(self, params, transformed=True, includes_fixed=False, complex_step=False)\n",
      " |      Update the parameters of the model\n",
      " |      \n",
      " |      Updates the representation matrices to fill in the new parameter\n",
      " |      values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of new parameters.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. If set to False,\n",
      " |          `transform_params` is called. Default is True..\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : array_like\n",
      " |          Array of parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.tsa.statespace.sarimax.SARIMAX:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables\n",
      " |  \n",
      " |  initial_design\n",
      " |      Initial design matrix\n",
      " |  \n",
      " |  initial_selection\n",
      " |      Initial selection matrix\n",
      " |  \n",
      " |  initial_state_intercept\n",
      " |      Initial state intercept vector\n",
      " |  \n",
      " |  initial_transition\n",
      " |      Initial transition matrix\n",
      " |  \n",
      " |  model_latex_names\n",
      " |      The latex names of all possible model parameters.\n",
      " |  \n",
      " |  model_names\n",
      " |      The plain text names of all possible model parameters.\n",
      " |  \n",
      " |  model_orders\n",
      " |      The orders of each of the polynomials in the model.\n",
      " |  \n",
      " |  param_names\n",
      " |      List of human readable parameter names (for parameters actually\n",
      " |      included in the model).\n",
      " |  \n",
      " |  param_terms\n",
      " |      List of parameters actually included in the model, in sorted order.\n",
      " |      \n",
      " |      TODO Make this an dict with slice or indices as the values.\n",
      " |  \n",
      " |  start_params\n",
      " |      Starting parameters for maximum likelihood estimation\n",
      " |  \n",
      " |  state_names\n",
      " |      (list of str) List of human readable names for unobserved states.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from statsmodels.tsa.statespace.sarimax.SARIMAX:\n",
      " |  \n",
      " |  params_complete = ['trend', 'exog', 'ar', 'ma', 'seasonal_ar', 'season...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.tsa.statespace.mlemodel.MLEModel:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type=None, cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs)\n",
      " |      Kalman filtering\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is True.\n",
      " |      return_ssm : bool,optional\n",
      " |          Whether or not to return only the state space output or a full\n",
      " |          results object. Default is to return a full results object.\n",
      " |      cov_type : str, optional\n",
      " |          See `MLEResults.fit` for a description of covariance matrix types\n",
      " |          for results object.\n",
      " |      cov_kwds : dict or None, optional\n",
      " |          See `MLEResults.get_robustcov_results` for a description required\n",
      " |          keywords for alternative covariance estimators\n",
      " |      low_memory : bool, optional\n",
      " |          If set to True, techniques are applied to substantially reduce\n",
      " |          memory usage. If used, some features of the results object will\n",
      " |          not be available (including in-sample prediction), although\n",
      " |          out-of-sample forecasting is possible. Default is False.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the Kalman filter. See\n",
      " |          `KalmanFilter.filter` for more details.\n",
      " |  \n",
      " |  fit_constrained(self, constraints, start_params=None, **fit_kwds)\n",
      " |      Fit the model with some parameters subject to equality constraints.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      constraints : dict\n",
      " |          Dictionary of constraints, of the form `param_name: fixed_value`.\n",
      " |          See the `param_names` property for valid parameter names.\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          If None, the default is given by Model.start_params.\n",
      " |      **fit_kwds : keyword arguments\n",
      " |          fit_kwds are used in the optimization of the remaining parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results : Results instance\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mod = sm.tsa.SARIMAX(endog, order=(1, 0, 1))\n",
      " |      >>> res = mod.fit_constrained({'ar.L1': 0.5})\n",
      " |  \n",
      " |  fix_params(self, params)\n",
      " |      Fix parameters to specific values (context manager)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Dictionary describing the fixed parameter values, of the form\n",
      " |          `param_name: fixed_value`. See the `param_names` property for valid\n",
      " |          parameter names.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mod = sm.tsa.SARIMAX(endog, order=(1, 0, 1))\n",
      " |      >>> with mod.fix_params({'ar.L1': 0.5}):\n",
      " |              res = mod.fit()\n",
      " |  \n",
      " |  handle_params(self, params, transformed=True, includes_fixed=False, return_jacobian=False)\n",
      " |      Ensure model parameters satisfy shape and other requirements\n",
      " |  \n",
      " |  hessian(self, params, *args, **kwargs)\n",
      " |      Hessian matrix of the likelihood function, evaluated at the given\n",
      " |      parameters\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the hessian.\n",
      " |      *args\n",
      " |          Additional positional arguments to the `loglike` method.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to the `loglike` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hessian : ndarray\n",
      " |          Hessian matrix evaluated at `params`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a numerical approximation.\n",
      " |      \n",
      " |      Both args and kwargs are necessary because the optimizer from\n",
      " |      `fit` must call this function and only supports passing arguments via\n",
      " |      args (for example `scipy.optimize.fmin_l_bfgs`).\n",
      " |  \n",
      " |  impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, **kwargs)\n",
      " |      Impulse response function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of model parameters.\n",
      " |      steps : int, optional\n",
      " |          The number of steps for which impulse responses are calculated.\n",
      " |          Default is 1. Note that for time-invariant models, the initial\n",
      " |          impulse is not counted as a step, so if `steps=1`, the output will\n",
      " |          have 2 entries.\n",
      " |      impulse : int, str or array_like\n",
      " |          If an integer, the state innovation to pulse; must be between 0\n",
      " |          and `k_posdef-1`. If a str, it indicates which column of df\n",
      " |          the unit (1) impulse is given.\n",
      " |          Alternatively, a custom impulse vector may be provided; must be\n",
      " |          shaped `k_posdef x 1`.\n",
      " |      orthogonalized : bool, optional\n",
      " |          Whether or not to perform impulse using orthogonalized innovations.\n",
      " |          Note that this will also affect custum `impulse` vectors. Default\n",
      " |          is False.\n",
      " |      cumulative : bool, optional\n",
      " |          Whether or not to return cumulative impulse responses. Default is\n",
      " |          False.\n",
      " |      anchor : int, str, or datetime, optional\n",
      " |          Time point within the sample for the state innovation impulse. Type\n",
      " |          depends on the index of the given `endog` in the model. Two special\n",
      " |          cases are the strings 'start' and 'end', which refer to setting the\n",
      " |          impulse at the first and last points of the sample, respectively.\n",
      " |          Integer values can run from 0 to `nobs - 1`, or can be negative to\n",
      " |          apply negative indexing. Finally, if a date/time index was provided\n",
      " |          to the model, then this argument can be a date string to parse or a\n",
      " |          datetime type. Default is 'start'.\n",
      " |      exog : array_like, optional\n",
      " |          New observations of exogenous regressors for our-of-sample periods,\n",
      " |          if applicable.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is\n",
      " |          True.\n",
      " |      includes_fixed : bool, optional\n",
      " |          If parameters were previously fixed with the `fix_params` method,\n",
      " |          this argument describes whether or not `params` also includes\n",
      " |          the fixed parameters, in addition to the free parameters. Default\n",
      " |          is False.\n",
      " |      **kwargs\n",
      " |          If the model has time-varying design or transition matrices and the\n",
      " |          combination of `anchor` and `steps` implies creating impulse\n",
      " |          responses for the out-of-sample period, then these matrices must\n",
      " |          have updated values provided for the out-of-sample steps. For\n",
      " |          example, if `design` is a time-varying component, `nobs` is 10,\n",
      " |          `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\n",
      " |          matrix must be provided with the new design matrix values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      impulse_responses : ndarray\n",
      " |          Responses for each endogenous variable due to the impulse\n",
      " |          given by the `impulse` argument. For a time-invariant model, the\n",
      " |          impulse responses are given for `steps + 1` elements (this gives\n",
      " |          the \"initial impulse\" followed by `steps` responses for the\n",
      " |          important cases of VAR and SARIMAX models), while for time-varying\n",
      " |          models the impulse responses are only given for `steps` elements\n",
      " |          (to avoid having to unexpectedly provide updated time-varying\n",
      " |          matrices).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      simulate\n",
      " |          Simulate a time series according to the given state space model,\n",
      " |          optionally with specified series for the innovations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Intercepts in the measurement and state equation are ignored when\n",
      " |      calculating impulse responses.\n",
      " |      \n",
      " |      TODO: add an option to allow changing the ordering for the\n",
      " |            orthogonalized option. Will require permuting matrices when\n",
      " |            constructing the extended model.\n",
      " |  \n",
      " |  initialize_approximate_diffuse(self, variance=None)\n",
      " |      Initialize approximate diffuse\n",
      " |  \n",
      " |  initialize_known(self, initial_state, initial_state_cov)\n",
      " |      Initialize known\n",
      " |  \n",
      " |  initialize_statespace(self, **kwargs)\n",
      " |      Initialize the state space representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the state space class\n",
      " |          constructor.\n",
      " |  \n",
      " |  initialize_stationary(self)\n",
      " |      Initialize stationary\n",
      " |  \n",
      " |  loglike(self, params, *args, **kwargs)\n",
      " |      Loglikelihood evaluation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is True.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the Kalman filter. See\n",
      " |          `KalmanFilter.filter` for more details.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      update : modifies the internal state of the state space model to\n",
      " |               reflect new params\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      [1]_ recommend maximizing the average likelihood to avoid scale issues;\n",
      " |      this is done automatically by the base Model fit method.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.\n",
      " |         Statistical Algorithms for Models in State Space Using SsfPack 2.2.\n",
      " |         Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.\n",
      " |  \n",
      " |  loglikeobs(self, params, transformed=True, includes_fixed=False, complex_step=False, **kwargs)\n",
      " |      Loglikelihood evaluation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is True.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the Kalman filter. See\n",
      " |          `KalmanFilter.filter` for more details.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      update : modifies the internal state of the Model to reflect new params\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      [1]_ recommend maximizing the average likelihood to avoid scale issues;\n",
      " |      this is done automatically by the base Model fit method.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.\n",
      " |         Statistical Algorithms for Models in State Space Using SsfPack 2.2.\n",
      " |         Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.\n",
      " |  \n",
      " |  observed_information_matrix(self, params, transformed=True, includes_fixed=False, approx_complex_step=None, approx_centered=False, **kwargs)\n",
      " |      Observed information matrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like, optional\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the Kalman filter. See\n",
      " |          `KalmanFilter.filter` for more details.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is from Harvey (1989), which shows that the information\n",
      " |      matrix only depends on terms from the gradient. This implementation is\n",
      " |      partially analytic and partially numeric approximation, therefore,\n",
      " |      because it uses the analytic formula for the information matrix, with\n",
      " |      numerically computed elements of the gradient.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Harvey, Andrew C. 1990.\n",
      " |      Forecasting, Structural Time Series Models and the Kalman Filter.\n",
      " |      Cambridge University Press.\n",
      " |  \n",
      " |  opg_information_matrix(self, params, transformed=True, includes_fixed=False, approx_complex_step=None, **kwargs)\n",
      " |      Outer product of gradients information matrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like, optional\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      **kwargs\n",
      " |          Additional arguments to the `loglikeobs` method.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Berndt, Ernst R., Bronwyn Hall, Robert Hall, and Jerry Hausman. 1974.\n",
      " |      Estimation and Inference in Nonlinear Structural Models.\n",
      " |      NBER Chapters. National Bureau of Economic Research, Inc.\n",
      " |  \n",
      " |  score(self, params, *args, **kwargs)\n",
      " |      Compute the score function at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the score.\n",
      " |      *args\n",
      " |          Additional positional arguments to the `loglike` method.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to the `loglike` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray\n",
      " |          Score, evaluated at `params`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a numerical approximation, calculated using first-order complex\n",
      " |      step differentiation on the `loglike` method.\n",
      " |      \n",
      " |      Both args and kwargs are necessary because the optimizer from\n",
      " |      `fit` must call this function and only supports passing arguments via\n",
      " |      args (for example `scipy.optimize.fmin_l_bfgs`).\n",
      " |  \n",
      " |  score_obs(self, params, method='approx', transformed=True, includes_fixed=False, approx_complex_step=None, approx_centered=False, **kwargs)\n",
      " |      Compute the score per observation, evaluated at params\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the score.\n",
      " |      **kwargs\n",
      " |          Additional arguments to the `loglike` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray\n",
      " |          Score per observation, evaluated at `params`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a numerical approximation, calculated using first-order complex\n",
      " |      step differentiation on the `loglikeobs` method.\n",
      " |  \n",
      " |  set_conserve_memory(self, conserve_memory=None, **kwargs)\n",
      " |      Set the memory conservation method\n",
      " |      \n",
      " |      By default, the Kalman filter computes a number of intermediate\n",
      " |      matrices at each iteration. The memory conservation options control\n",
      " |      which of those matrices are stored.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conserve_memory : int, optional\n",
      " |          Bitmask value to set the memory conservation method to. See notes\n",
      " |          for details.\n",
      " |      **kwargs\n",
      " |          Keyword arguments may be used to influence the memory conservation\n",
      " |          method by setting individual boolean flags.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is rarely used. See the corresponding function in the\n",
      " |      `KalmanFilter` class for details.\n",
      " |  \n",
      " |  set_filter_method(self, filter_method=None, **kwargs)\n",
      " |      Set the filtering method\n",
      " |      \n",
      " |      The filtering method controls aspects of which Kalman filtering\n",
      " |      approach will be used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filter_method : int, optional\n",
      " |          Bitmask value to set the filter method to. See notes for details.\n",
      " |      **kwargs\n",
      " |          Keyword arguments may be used to influence the filter method by\n",
      " |          setting individual boolean flags. See notes for details.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is rarely used. See the corresponding function in the\n",
      " |      `KalmanFilter` class for details.\n",
      " |  \n",
      " |  set_inversion_method(self, inversion_method=None, **kwargs)\n",
      " |      Set the inversion method\n",
      " |      \n",
      " |      The Kalman filter may contain one matrix inversion: that of the\n",
      " |      forecast error covariance matrix. The inversion method controls how and\n",
      " |      if that inverse is performed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inversion_method : int, optional\n",
      " |          Bitmask value to set the inversion method to. See notes for\n",
      " |          details.\n",
      " |      **kwargs\n",
      " |          Keyword arguments may be used to influence the inversion method by\n",
      " |          setting individual boolean flags. See notes for details.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is rarely used. See the corresponding function in the\n",
      " |      `KalmanFilter` class for details.\n",
      " |  \n",
      " |  set_smoother_output(self, smoother_output=None, **kwargs)\n",
      " |      Set the smoother output\n",
      " |      \n",
      " |      The smoother can produce several types of results. The smoother output\n",
      " |      variable controls which are calculated and returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      smoother_output : int, optional\n",
      " |          Bitmask value to set the smoother output to. See notes for details.\n",
      " |      **kwargs\n",
      " |          Keyword arguments may be used to influence the smoother output by\n",
      " |          setting individual boolean flags.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is rarely used. See the corresponding function in the\n",
      " |      `KalmanSmoother` class for details.\n",
      " |  \n",
      " |  set_stability_method(self, stability_method=None, **kwargs)\n",
      " |      Set the numerical stability method\n",
      " |      \n",
      " |      The Kalman filter is a recursive algorithm that may in some cases\n",
      " |      suffer issues with numerical stability. The stability method controls\n",
      " |      what, if any, measures are taken to promote stability.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      stability_method : int, optional\n",
      " |          Bitmask value to set the stability method to. See notes for\n",
      " |          details.\n",
      " |      **kwargs\n",
      " |          Keyword arguments may be used to influence the stability method by\n",
      " |          setting individual boolean flags. See notes for details.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is rarely used. See the corresponding function in the\n",
      " |      `KalmanFilter` class for details.\n",
      " |  \n",
      " |  simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, pretransformed_measurement_shocks=True, pretransformed_state_shocks=True, pretransformed_initial_state=True, random_state=None, **kwargs)\n",
      " |      Simulate a new time series following the state space model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters to use in constructing the state space\n",
      " |          representation to use when simulating.\n",
      " |      nsimulations : int\n",
      " |          The number of observations to simulate. If the model is\n",
      " |          time-invariant this can be any number. If the model is\n",
      " |          time-varying, then this number must be less than or equal to the\n",
      " |          number of observations.\n",
      " |      measurement_shocks : array_like, optional\n",
      " |          If specified, these are the shocks to the measurement equation,\n",
      " |          :math:`\\varepsilon_t`. If unspecified, these are automatically\n",
      " |          generated using a pseudo-random number generator. If specified,\n",
      " |          must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\n",
      " |          same as in the state space model.\n",
      " |      state_shocks : array_like, optional\n",
      " |          If specified, these are the shocks to the state equation,\n",
      " |          :math:`\\eta_t`. If unspecified, these are automatically\n",
      " |          generated using a pseudo-random number generator. If specified,\n",
      " |          must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\n",
      " |          same as in the state space model.\n",
      " |      initial_state : array_like, optional\n",
      " |          If specified, this is the initial state vector to use in\n",
      " |          simulation, which should be shaped (`k_states` x 1), where\n",
      " |          `k_states` is the same as in the state space model. If unspecified,\n",
      " |          but the model has been initialized, then that initialization is\n",
      " |          used. This must be specified if `anchor` is anything other than\n",
      " |          \"start\" or 0 (or else you can use the `simulate` method on a\n",
      " |          results object rather than on the model object).\n",
      " |      anchor : int, str, or datetime, optional\n",
      " |          First period for simulation. The simulation will be conditional on\n",
      " |          all existing datapoints prior to the `anchor`.  Type depends on the\n",
      " |          index of the given `endog` in the model. Two special cases are the\n",
      " |          strings 'start' and 'end'. `start` refers to beginning the\n",
      " |          simulation at the first period of the sample, and `end` refers to\n",
      " |          beginning the simulation at the first period after the sample.\n",
      " |          Integer values can run from 0 to `nobs`, or can be negative to\n",
      " |          apply negative indexing. Finally, if a date/time index was provided\n",
      " |          to the model, then this argument can be a date string to parse or a\n",
      " |          datetime type. Default is 'start'.\n",
      " |      repetitions : int, optional\n",
      " |          Number of simulated paths to generate. Default is 1 simulated path.\n",
      " |      exog : array_like, optional\n",
      " |          New observations of exogenous regressors, if applicable.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is\n",
      " |          True.\n",
      " |      includes_fixed : bool, optional\n",
      " |          If parameters were previously fixed with the `fix_params` method,\n",
      " |          this argument describes whether or not `params` also includes\n",
      " |          the fixed parameters, in addition to the free parameters. Default\n",
      " |          is False.\n",
      " |      pretransformed_measurement_shocks : bool, optional\n",
      " |          If `measurement_shocks` is provided, this flag indicates whether it\n",
      " |          should be directly used as the shocks. If False, then it is assumed\n",
      " |          to contain draws from the standard Normal distribution that must be\n",
      " |          transformed using the `obs_cov` covariance matrix. Default is True.\n",
      " |      pretransformed_state_shocks : bool, optional\n",
      " |          If `state_shocks` is provided, this flag indicates whether it\n",
      " |          should be directly used as the shocks. If False, then it is assumed\n",
      " |          to contain draws from the standard Normal distribution that must be\n",
      " |          transformed using the `state_cov` covariance matrix. Default is\n",
      " |          True.\n",
      " |      pretransformed_initial_state : bool, optional\n",
      " |          If `initial_state` is provided, this flag indicates whether it\n",
      " |          should be directly used as the initial_state. If False, then it is\n",
      " |          assumed to contain draws from the standard Normal distribution that\n",
      " |          must be transformed using the `initial_state_cov` covariance\n",
      " |          matrix. Default is True.\n",
      " |      random_state : {None, int, Generator, RandomState}, optional\n",
      " |          If `seed` is None (or `np.random`), the\n",
      " |          class:``~numpy.random.RandomState`` singleton is used.\n",
      " |          If `seed` is an int, a new class:``~numpy.random.RandomState``\n",
      " |          instance is used, seeded with `seed`.\n",
      " |          If `seed` is already a class:``~numpy.random.Generator`` or\n",
      " |          class:``~numpy.random.RandomState`` instance then that instance is\n",
      " |          used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      simulated_obs : ndarray\n",
      " |          An array of simulated observations. If `repetitions=None`, then it\n",
      " |          will be shaped (nsimulations x k_endog) or (nsimulations,) if\n",
      " |          `k_endog=1`. Otherwise it will be shaped\n",
      " |          (nsimulations x k_endog x repetitions). If the model was given\n",
      " |          Pandas input then the output will be a Pandas object. If\n",
      " |          `k_endog > 1` and `repetitions` is not None, then the output will\n",
      " |          be a Pandas DataFrame that has a MultiIndex for the columns, with\n",
      " |          the first level containing the names of the `endog` variables and\n",
      " |          the second level containing the repetition number.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      impulse_responses\n",
      " |          Impulse response functions\n",
      " |  \n",
      " |  simulation_smoother(self, simulation_output=None, **kwargs)\n",
      " |      Retrieve a simulation smoother for the state space model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      simulation_output : int, optional\n",
      " |          Determines which simulation smoother output is calculated.\n",
      " |          Default is all (including state and disturbances).\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments, used to set the simulation output.\n",
      " |          See `set_simulation_output` for more details.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      SimulationSmoothResults\n",
      " |  \n",
      " |  smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type=None, cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs)\n",
      " |      Kalman smoothing\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Array of parameters at which to evaluate the loglikelihood\n",
      " |          function.\n",
      " |      transformed : bool, optional\n",
      " |          Whether or not `params` is already transformed. Default is True.\n",
      " |      return_ssm : bool,optional\n",
      " |          Whether or not to return only the state space output or a full\n",
      " |          results object. Default is to return a full results object.\n",
      " |      cov_type : str, optional\n",
      " |          See `MLEResults.fit` for a description of covariance matrix types\n",
      " |          for results object.\n",
      " |      cov_kwds : dict or None, optional\n",
      " |          See `MLEResults.get_robustcov_results` for a description required\n",
      " |          keywords for alternative covariance estimators\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass to the Kalman filter. See\n",
      " |          `KalmanFilter.filter` for more details.\n",
      " |  \n",
      " |  transform_jacobian(self, unconstrained, approx_centered=False)\n",
      " |      Jacobian matrix for the parameter transformation function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      unconstrained : array_like\n",
      " |          Array of unconstrained parameters used by the optimizer.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      jacobian : ndarray\n",
      " |          Jacobian matrix of the transformation, evaluated at `unconstrained`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      transform_params\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a numerical approximation using finite differences. Note that\n",
      " |      in general complex step methods cannot be used because it is not\n",
      " |      guaranteed that the `transform_params` method is a real function (e.g.\n",
      " |      if Cholesky decomposition is used).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.tsa.statespace.mlemodel.MLEModel:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None) from builtins.type\n",
      " |      Not implemented for state space models\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.tsa.statespace.mlemodel.MLEModel:\n",
      " |  \n",
      " |  initial_variance\n",
      " |  \n",
      " |  initialization\n",
      " |  \n",
      " |  loglikelihood_burn\n",
      " |  \n",
      " |  tolerance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel:\n",
      " |  \n",
      " |  exog_names\n",
      " |      The names of the exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  predict(self, params, exog=None, *args, **kwargs)\n",
      " |      After a model has been fit predict returns the fitted values.\n",
      " |      \n",
      " |      This is a placeholder intended to be overwritten by individual models.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "help(ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d7e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols=None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters=None, true_values=None, false_values=None, skipinitialspace: 'bool' = False, skiprows=None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values=None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool' = False, date_parser=<no_default>, date_format: 'str | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool' = False, low_memory=True, memory_map: 'bool' = False, float_precision: \"Literal[('high', 'legacy')] | None\" = None, storage_options: 'StorageOptions' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, None, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "    \n",
      "            Support for defaultdict was added. Specify a defaultdict as input where\n",
      "            the default determines the dtype of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True in addition to case-insensitive variants of \"True\".\n",
      "    false_values : list, optional\n",
      "        Values to consider as False in addition to case-insensitive variants of \"False\".\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n",
      "        'n/a', 'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    \n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "    \n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    \n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, default ``None``\n",
      "       If used in conjunction with ``parse_dates``, will parse dates according to this\n",
      "       format. For anything more complex,\n",
      "       please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n",
      "    \n",
      "       .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
      "        key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "    \n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "    \n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default \"utf-8\"\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    \n",
      "        .. versionchanged:: 1.3.0\n",
      "    \n",
      "           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "           influence on how encoding errors are handled.\n",
      "    \n",
      "    encoding_errors : str, optional, default \"strict\"\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "            - 'error', raise an Exception when a bad line is encountered.\n",
      "            - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "            - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            - callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new list of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine=\"python\"``\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    dtype_backend : {\"numpy_nullable\", \"pyarrow\"}, defaults to NumPy backed DataFrames\n",
      "        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy\n",
      "        arrays, nullable dtypes are used for all dtypes that have a nullable\n",
      "        implementation when \"numpy_nullable\" is set, pyarrow is used for all\n",
      "        dtypes if \"pyarrow\" is set.\n",
      "    \n",
      "        The dtype_backends are still experimential.\n",
      "    \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "help(read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b03ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARIMA",
   "language": "python",
   "name": "arima"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
